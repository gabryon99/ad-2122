% !TeX program = lualatex

\documentclass{article}
\usepackage[a4paper, total={5.5in, 10in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{calrsfs}
\usepackage{lmodern}%get scalable font
\usepackage{titling}
\usepackage{listings}
\usepackage{fontspec}
\usepackage{lstfiracode}

% Define command for show :=
\newcommand{\defeq}{\vcentcolon=}
% Set font family
\fontfamily{qpl}\selectfont 
% Calligraphic H
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
\newcommand{\Hb}{\pazocal{H}}
\newcommand{\Bb}{\pazocal{B}}

\newcommand*{\QEDB}{\null\nobreak\hfill\ensuremath{\square}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition*}[theorem]{Definition}

\title{Second hands-on: Depth of a node in a Random Search Tree\\[1ex] \large Algorithm Design (2021/2022)}
\author{Gabriele Pappalardo\\Email: g.pappalardo4@studenti.unipi.it\\Department of Computer Science}
\date{March 2022}

\pretitle{
    \begin{center}
    \LARGE
    \includegraphics[width=5cm]{/Users/gabryon/Projects/Papers/AD-21-22/assets/images/unipi.png}\\
}
\posttitle{
    \end{center}
}

\setmonofont{FiraCode}[
    Scale=0.85,
    Contextuals=Alternate % Activate the calt feature
]

\lstset{
    style=FiraCodeStyle,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                
    showspaces=false,                
    showstringspaces=false,
    basewidth=0.6em,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    basicstyle=\ttfamily, % Use \ttfamily for source code listings
    commentstyle=\color{ForestGreen}
}

\setcounter{section}{0}

\begin{document}

\maketitle

\section{Introduction}

A \textbf{random binary search tree} for a set $S$ can be defined as follows: if $S$ is empty, then the null tree is a random search tree; otherwise, choose uniformly at random a key $k \in S$: the random search tree is obtained by picking $k$ as \textit{root}, and the random search trees on $L = \{x \in S : x < k\}$ and $R = \{x \in S : x > k\}$ become, respectively, the left and right subtrees of the root $k$.\\

\noindent Consider the \textit{Randomized Quick Sort} (RQS) discussed in class and analyzed with indicator variables, and observe that the random selection of the pivots follows the above process, thus producing a random search tree of $n$ nodes.

\begin{itemize}
    \item Using a variation of the analysis with indicator variables $X_{ij}$, prove that the expected depth of a node (i.e. the random variable representing the distance of the node from the root) is nearly $2 \log n$. 
    \item Prove that the expected size of its subtree is nearly $2 \log n$ too, observing that it is a simple variation of the previous analysis.
    \item Prove that the probability that the depth of a node exceeds $c2 \log n$ is small for any given constant $c > 2$. 
\end{itemize}

\section{Solution}

The hands-on's solution relies on the analysis we did during the lectures with randomized version of Quick-Sort.
Let us recall for a moment how the algorithm works.

\begin{lstlisting}[language=Python,caption=`Randomized QuickSort']
def randomizedPartition(A: [int], p: int, r: int) -> int:
    i = random.randint(p, r)
    swap(A[r], A[i])
    return partition(A, p, r)

def randomizedQuickSort(A: [int], p: int, r: int):
    if p < r:
        q = randomizedPartition(A, p, r)
        randomizedQuickSort(A, p, q - 1)
        randomizedQuickSort(A, q + 1, r)
\end{lstlisting}

\noindent At the first sight of the code it seems that an RBST and the RQS do have nothing in common. Actually,
the way how the RBST is build match 1-to-1 to the recursion tree created by the RQS.\\

\noindent When we select a random key $k \in S$ during the building of a RBST (the root of our tree), it corresponds to select a pivot 
in RQS. And the partition call allows us to create two subtrees ($L, R$) that will have elements less/greater than pivot,
and then we call the tree build construction algorithm onto the subtrees.\\ 

\subsection{Expected depth of a node}

We define the elements of a random binary search tree with $\forall i \in [1, n] . x_i \in S$. Let $X_{ij}$ an indicator random variable defined as follows:
\begin{equation}
    X_{ij} = \begin{cases}
    1 & x_i \, \textrm{is an ancestor of} \, x_j\\
    0 & \textrm{otherwise}
    \end{cases}
\end{equation}

\noindent The depth of a node can be expressed as the sum of all the indicator random variables. Let $D_i$ be the depth of a node $x_i$ in a random binary search tree, defined as $D_i = \sum_{j = 1}^{n} X_{ij}$. We can express the expected depth as:
\begin{equation}
E[D_i] = E\bigg[\sum_{j = 1}^{n} X_{ij}\bigg] = \sum_{j = 1}^{n} E[X_{ij}] = \sum_{j = 1}^{n} \textrm{Pr}(\{X_{ij} = 1\})
\end{equation}

\noindent The probability of $X_{ij} = 1$ can be computed using the analysis we did during lectures with RQS. Since the RQS maps
over the RBST we can say that $X_{ij} = 1$ when $x_i$ is a pivot (select as root for a tree) or $x_j$ is a pivot.\\

\noindent The expected depth of a node can be computed using this fact:

\begin{align*}
    E[D_i] = \sum_{j = 0}^{n} \textrm{Pr}(\{X_{ij} = 1\}) = \\ 
    \sum_{j = 0}^{i} \frac{1}{i - j + 1} + \sum_{j = i + 1}^{n} \frac{1}{j - i + 1} = \\
    \sum_{k = 1}^{i + 1} \frac{1}{k} + \sum_{k = 2}^{n - i + 1} \frac{1}{k} \le 
    \log{n} + \log{n} = 2 \log{n}
\end{align*}

\subsection{Expected size of a subtree}

The analysis for the expected size of a subtree is similar to the previous one. We change the meaning of our indicator random variable as:
\begin{equation}
    X_{ij} = \begin{cases}
    1 & x_i \, \textrm{is an descendant of} \, x_j\\
    0 & \textrm{otherwise}
    \end{cases}
\end{equation}

\subsection{Probability for the depth of a node}

\noindent Knowing that the expected depth of a node $E[D_i] \le 2 \log n$, we can prove that probability that the depth
of a node exceeds $c 2 \log n$ is small for any given constant $c \ge 2$.

\begin{align}
    \textrm{\textbf{Pr}}(\{D_i \ge c 2 \log n\}) \le e^{-\frac{\lambda^2}{2 \mu + \lambda}}
\end{align}

\noindent Knowing that $\mu = E[D_i] \le 2 \log n$ we can derive $\lambda$:

\begin{align*}
    \mu + \lambda = c 2 \log n \\
    E[D_i] + \lambda = c 2 \log n \\
    \lambda = c 2 \log n - E[D_i] \le c2 \log n - 2 \log n = \log n(2c - 2)
\end{align*}

\noindent Therefore, the Chernoff Bound becomes:

\begin{align*}
    \textrm{\textbf{Pr}}(\{D_i \ge c 2 \log n\}) \le e^{-\frac{(\log n(2c - 2))^2}{2(2 \log n) + \log n(2c - 2)}} \\
    = e^{-\frac{(\log n)^2 (2c-2)^2}{2(2 \log n) + \log n (2c - 2)}} \\
    = e^{-\frac{(\log n)^2 (2c-2)^2}{\log n (2c + 2)}} \\
    = e^{-\frac{\log n (2c-2)^2}{(2c + 2)}} \\
    = n^{-\frac{(2c-2)^2}{(2c + 2)}} \\
    = n^{-\frac{4c^2 + 4 - 8c}{2c + 2}} \\
    = n^{-\frac{2c^2 + 2 - 4c}{c + 1}} \\
    = \frac{1}{n^{\frac{2c^2 + 2 - 4c}{c + 1}}}
\end{align*}

At the end, the probability is small for any constant $c > 2$.

\end{document}
